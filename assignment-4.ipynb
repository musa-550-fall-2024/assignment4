{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Street Networks & Web Scraping & Text Analytics\n",
    "\n",
    "**Part 1: Visualizing crash data in Philadelphia**\n",
    "\n",
    "In this section, you will use `osmnx` to analyze the crash incidence in Center City. \n",
    "\n",
    "\n",
    "**Part 2: Scraping Craigslist**\n",
    "\n",
    "In this section, you will use Selenium and BeautifulSoup to scrape data for hundreds of apartments from Philadelphia's Craigslist portal.\n",
    "\n",
    "\n",
    "**Part 3: Text Analytics**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1: Visualizing crash data in Philadelphia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1 Load the geometry for the region being analyzed\n",
    "\n",
    "We'll analyze crashes in the \"Central\" planning district in Philadelphia, a rough approximation for Center City. [Planning districts](https://www.opendataphilly.org/dataset/planning-districts) can be loaded from Open Data Philly. Read the data into a GeoDataFrame using the following link:\n",
    "\n",
    "[http://data.phl.opendata.arcgis.com/datasets/0960ea0f38f44146bb562f2b212075aa_0.geojson](http://data.phl.opendata.arcgis.com/datasets/0960ea0f38f44146bb562f2b212075aa_0.geojson)\n",
    "\n",
    "Select the \"Central\" district and extract the geometry polygon for only this district. After this part, you should have a polygon variable of type `shapely.geometry.polygon.Polygon`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2 Get the street network graph\n",
    "\n",
    "Use OSMnx to create a network graph (of type 'drive') from your polygon boundary in 1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.3 Convert your network graph edges to a GeoDataFrame\n",
    "\n",
    "Use OSMnx to create a GeoDataFrame of the network edges in the graph object from part 1.2. The GeoDataFrame should contain the edges but not the nodes from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.4 Load PennDOT crash data\n",
    "\n",
    "Data for crashes (of all types) for 2020, 2021, and 2022 in Philadelphia County is available at the following path:\n",
    "\n",
    "`./data/CRASH_PHILADELPHIA_XXXX.csv`\n",
    "\n",
    "You should see three separate files in the `data/` folder. Use pandas to read each of the CSV files, and combine them into a single dataframe using `pd.concat()`.\n",
    "\n",
    "The data was downloaded for Philadelphia County [from here](https://crashinfo.penndot.gov/PCIT/welcome.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.5 Convert the crash data to a GeoDataFrame\n",
    "\n",
    "You will need to use the `DEC_LAT` and `DEC_LONG` columns for latitude and longitude.\n",
    "\n",
    "The full data dictionary for the data is [available here](http://pennshare.maps.arcgis.com/sharing/rest/content/items/ffe20c6c3c594389b275c6772a281bcd/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.6 Trim the crash data to Center City\n",
    "\n",
    "1. Get the boundary of the edges data frame (from part 1.3). Accessing the `.geometry.unary_union.convex_hull` property will give you a nice outer boundary region.\n",
    "1. Trim the crashes using the `within()` function of the crash GeoDataFrame to find which crashes are within the boundary.\n",
    "\n",
    "There should be about 3,750 crashes within the Central district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Re-project our data into an approriate CRS\n",
    "\n",
    "We'll need to find the nearest edge (street) in our graph for each crash. To do this, `osmnx` will calculate the distance from each crash to the graph edges. For this calculation to be accurate, we need to convert from latitude/longitude \n",
    "\n",
    "**We'll convert the local state plane CRS for Philadelphia, EPSG=2272**\n",
    "\n",
    "#### Two steps:\n",
    "1. Project the graph object (`G`) using the `ox.project_graph`. Run `ox.project_graph?` to see the documentation for how to convert to a specific CRS. \n",
    "1. Project the crash data using the `.to_crs()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.8 Find the nearest edge for each crash\n",
    "\n",
    "See: `ox.distance.nearest_edges()`. It takes three arguments:\n",
    "\n",
    "- the network graph\n",
    "- the longitude of your crash data (the `x` attribute of the `geometry` column)\n",
    "- the latitude of your crash data (the `y` attribute of the `geometry` column)\n",
    "\n",
    "You will get a numpy array with 3 columns that represent `(u, v, key)` where each `u` and `v` are the node IDs that the edge links together. We will ignore the `key` value for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.9 Calculate the total number of crashes per street\n",
    "\n",
    "1. Make a DataFrame from your data from part 1.7 with three columns, `u`, `v`, and `key` (we will only use the `u` and `v` columns)\n",
    "1. Group by `u` and `v` and calculate the size\n",
    "1. Reset the index and name your `size()` column as `crash_count`\n",
    "\n",
    "After this step you should have a DataFrame with three columns: `u`, `v`, and `crash_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.10 Merge your edges GeoDataFrame and crash count DataFrame\n",
    "\n",
    "You can use pandas to merge them on the `u` and `v` columns. This will associate the total crash count with each edge in the street network. \n",
    "\n",
    "**Tips:** \n",
    "   - Use a `left` merge where the first argument of the merge is the edges GeoDataFrame. This ensures no edges are removed during the merge.\n",
    "   - Use the `fillna(0)` function to fill in missing crash count values with zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.11 Calculate a \"Crash Index\"\n",
    "\n",
    "Let's calculate a \"crash index\" that provides a normalized measure of the crash frequency per street. To do this, we'll need to:\n",
    "\n",
    "1. Calculate the total crash count divided by the street length, using the `length` column\n",
    "1. Perform a log transformation of the crash/length variable — use numpy's `log10()` function\n",
    "1. Normalize the index from 0 to 1 (see the lecture notes for an example of this transformation)\n",
    "\n",
    "**Note: since the crash index involves a log transformation, you should only calculate the index for streets where the crash count is greater than zero**.\n",
    "\n",
    "After this step, you should have a new column in the data frame from 1.9 that includes a column called part 1.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12 Plot a histogram of the crash index values\n",
    "\n",
    "Use matplotlib's `hist()` function to plot the crash index values from the previous step.\n",
    "\n",
    "You should see that the index values are Gaussian-distributed, providing justification for why we log-transformed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.13 Plot an interactive map of the street networks, colored by the crash index\n",
    "\n",
    "You can use GeoPandas to make an interactive Folium map, coloring the streets by the crash index column.\n",
    "\n",
    "**Tip:** if you use the viridis color map, try using a \"dark\" tile set for better constrast of the colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Scraping Craigslist\n",
    "\n",
    "In this part, we'll be extracting information on apartments from Craigslist search results. You'll be using Selenium and BeautifulSoup to extract the relevant information from the HTML text. \n",
    "\n",
    "For reference on CSS selectors, please see the [notes from Week 6](https://github.com/MUSA-550-Fall-2022/week-6/blob/main/css-selectors.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primer: the Craigslist website URL\n",
    "\n",
    "We'll start with the Philadelphia region. First we need to figure out how to submit a query to Craigslist. As with many websites, one way you can do this is simply by constructing the proper URL and sending it to Craigslist. \n",
    "\n",
    "[https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1~gallery~0~0](https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1~gallery~0~0)\n",
    "\n",
    "There are **three** components to this URL. \n",
    "\n",
    "1. The base URL: `http://philadelphia.craigslist.org/search/apa`\n",
    "\n",
    "2. The user's search parameters: `?min_price=1&min_bedrooms=1&minSqft=1`\n",
    "\n",
    "> We will send nonzero defaults for some parameters (bedrooms, size, price) in order to exclude results that have empty values for these parameters.\n",
    "\n",
    "\n",
    "3. The URL *hash*: `#search=1~gallery~0~0`\n",
    "\n",
    "> As we will see later, this part will be important because it contains the search page result number.\n",
    "\n",
    "\n",
    "The Craigslist website requires Javascript, so we'll need to use Selenium to load the page, and then use BeautifulSoup to extract the information we want. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Initialize a selenium driver and open Craigslist\n",
    "\n",
    "As discussed in lecture, you can use Chrome, Firefox, or Edge as your selenium driver. In this part, you should do two things:\n",
    "\n",
    "1. Initialize the selenium driver\n",
    "1. Use the `driver.get()` function to open the following URL:\n",
    "\n",
    "[https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1~gallery~0~0](https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1~gallery~0~0)\n",
    "\n",
    "This will give you the search results for 1-bedroom apartments in Philadelphia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Obtaining dependency information for selenium from https://files.pythonhosted.org/packages/91/08/10cff8463b3510b78f9e3dcef6b37c542b06d71ed1240a8940ba0c75d3bc/selenium-4.26.1-py3-none-any.whl.metadata\n",
      "  Downloading selenium-4.26.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /Users/delmelle/miniforge3/lib/python3.10/site-packages (from selenium) (2.0.4)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Obtaining dependency information for trio~=0.17 from https://files.pythonhosted.org/packages/3c/83/ec3196c360afffbc5b342ead48d1eb7393dd74fa70bca75d33905a86f211/trio-0.27.0-py3-none-any.whl.metadata\n",
      "  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Obtaining dependency information for trio-websocket~=0.9 from https://files.pythonhosted.org/packages/48/be/a9ae5f50cad5b6f85bd2574c2c923730098530096e170c1ce7452394d7aa/trio_websocket-0.11.1-py3-none-any.whl.metadata\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Users/delmelle/miniforge3/lib/python3.10/site-packages (from selenium) (2024.7.4)\n",
      "Collecting typing_extensions~=4.9 (from selenium)\n",
      "  Obtaining dependency information for typing_extensions~=4.9 from https://files.pythonhosted.org/packages/26/9f/ad63fc0248c5379346306f8668cda6e2e2e9c95e01216d2b8ffd9ff037d0/typing_extensions-4.12.2-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting websocket-client~=1.8 (from selenium)\n",
      "  Obtaining dependency information for websocket-client~=1.8 from https://files.pythonhosted.org/packages/5a/84/44687a29792a70e111c5c477230a72c4b957d88d16141199bf9acb7537a3/websocket_client-1.8.0-py3-none-any.whl.metadata\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Obtaining dependency information for attrs>=23.2.0 from https://files.pythonhosted.org/packages/6a/21/5b6702a7f963e95456c0de2d495f67bf5fd62840ac655dc451586d23d39a/attrs-24.2.0-py3-none-any.whl.metadata\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: sortedcontainers in /Users/delmelle/miniforge3/lib/python3.10/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /Users/delmelle/miniforge3/lib/python3.10/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Obtaining dependency information for outcome from https://files.pythonhosted.org/packages/55/8b/5ab7257531a5d830fc8000c476e63c935488d74609b50f9384a643ec0a62/outcome-1.3.0.post0-py2.py3-none-any.whl.metadata\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /Users/delmelle/miniforge3/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /Users/delmelle/miniforge3/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Obtaining dependency information for wsproto>=0.14 from https://files.pythonhosted.org/packages/78/58/e860788190eba3bcce367f74d29c4675466ce8dddfba85f7827588416f01/wsproto-1.2.0-py3-none-any.whl.metadata\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /Users/delmelle/miniforge3/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Obtaining dependency information for h11<1,>=0.9.0 from https://files.pythonhosted.org/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl.metadata\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Downloading selenium-4.26.1-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: websocket-client, typing_extensions, h11, attrs, wsproto, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: websocket-client\n",
      "    Found existing installation: websocket-client 1.7.0\n",
      "    Uninstalling websocket-client-1.7.0:\n",
      "      Successfully uninstalled websocket-client-1.7.0\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "geoviews 1.13.0 requires bokeh<3.6.0,>=3.5.0, but you have bokeh 2.4.3 which is incompatible.\n",
      "holoviews 1.19.1 requires bokeh>=3.1, but you have bokeh 2.4.3 which is incompatible.\n",
      "jupyter-server 2.12.5 requires tornado>=6.2.0, but you have tornado 6.1 which is incompatible.\n",
      "jupyterlab 4.0.12 requires tornado>=6.2.0, but you have tornado 6.1 which is incompatible.\n",
      "notebook 7.0.7 requires tornado>=6.2.0, but you have tornado 6.1 which is incompatible.\n",
      "panel 1.5.0 requires bokeh<3.6.0,>=3.5.0, but you have bokeh 2.4.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed attrs-24.2.0 h11-0.14.0 outcome-1.3.0.post0 selenium-4.26.1 trio-0.27.0 trio-websocket-0.11.1 typing_extensions-4.12.2 websocket-client-1.8.0 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium \n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1gallery0~0\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Initialize your \"soup\"\n",
    "\n",
    "Once selenium has the page open, we can get the page source from the driver and use BeautifulSoup to parse it. In this part, initialize a BeautifulSoup object with the driver's page source\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "propertySoup = BeautifulSoup(driver.page_source, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Parsing the HTML\n",
    "\n",
    "Now that we have our \"soup\" object, we can use BeautifulSoup to extract out the elements we need:\n",
    "\n",
    "- Use the Web Inspector to identify the HTML element that holds the information on each apartment listing.\n",
    "- Use BeautifulSoup to extract these elements from the HTML. \n",
    "\n",
    "\n",
    "At the end of this part, you should have a list of 120 elements, where each element is the listing for a specific apartment on the search page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<li class=\"cl-search-result cl-search-view-mode-gallery\" data-pid=\"7800059598\" title=\"Beautiful Single family Home - Cedar Park University City\">\n",
      " <div class=\"gallery-card\">\n",
      "  <div class=\"cl-gallery\">\n",
      "   <div class=\"gallery-inner\">\n",
      "    <a class=\"main\" href=\"https://philadelphia.craigslist.org/apa/d/philadelphia-beautiful-single-family/7800059598.html\">\n",
      "     <div class=\"swipe\" style=\"visibility: visible;\">\n",
      "      <div class=\"swipe-wrap\" style=\"width: 10336px;\">\n",
      "       <div data-index=\"0\" style=\"width: 304px; left: 0px; transition-duration: 0ms; transform: translateX(0px);\">\n",
      "        <span class=\"loading icom-\">\n",
      "        </span>\n",
      "        <img alt=\"Beautiful Single family Home - Cedar Park University City 1\" data-image-index=\"0\" src=\"https://images.craigslist.org/00N0N_kgvrS202Ypp_0t20CI_600x450.jpg\"/>\n",
      "       </div>\n",
      "       <div data-index=\"1\" style=\"width: 304px; left: -304px; transition-duration: 0ms; transform: translateX(304px);\">\n",
      "       </div>\n",
      "       <div data-index=\"2\" style=\"width: 304px; left: -608px; transition-duration: 0ms; transform: translateX(304px);\">\n",
      "       </div>\n",
      "       <div data-index=\"3\" style=\"width: 304px; left: -912px; transition-duration: 0ms; transform: translateX(304px);\">\n",
      "       </div>\n",
      "       <div data-index=\"4\" style=\"width: 304px; left: -1216px; transition-duration: 0ms; transform: translateX(304px);\">\n",
      "       </div>\n",
      "       <div data-index=\"5\" style=\"width: 304px; left: -1520px; transition-duration: 0ms; transform: translateX(304px);\">\n",
      "       </div>\n",
      "       <div data-index=\"6\" style=\"width: 304px; left: -1824px; transition-duration: 0ms; transform: translateX(304px);\">\n",
      "       </div>\n",
      "       <div data-index=\"7\" style=\"width: 304px; left: -2128px; transition-duration: 0ms; transform: translateX(304px);\">\n",
      "       </div>\n",
      "       <div data-index=\"8\" style=\"width: 304px; left: -2432px; transition-duration: 0ms; transform: translateX(304px);\">\n",
      "       </div>\n",
      "       <div data-index=\"9\" style=\"width: 304px; left: -2736px; transition-duration: 0ms; transform: translateX(304px);\">\n",
      "       </div>\n",
      "       <div data-index=\"10\" style=\"width: 304px; left: -3040px; transition-duration: 0ms; transform: translateX(304px);\">\n",
      "       </div>\n",
      "       <div data-index=\"11\" style=\"width: 304px; left: -3344px; transition-duration: 0ms; transform: translateX(304px);\">\n",
      "       </div>\n",
      "       <div data-index=\"12\" style=\"width: 304px; left: -3648px; transition-duration: 0ms; transform: translateX(304px);\">\n",
      "       </div>\n",
      "       <div data-index=\"13\" style=\"width: 304px; left: -3952px; transition-duration: 0ms; transform: translateX(304px);\">\n",
      "       </div>\n",
      "       <div data-index=\"14\" style=\"width: 304px; left: -4256px; transition-duration: 0ms; transform: translateX(304px);\">\n",
      "       </div>\n",
      "       <div data-index=\"15\" style=\"width: 304px; left: -4560px; transition-duration: 0ms; transform: translateX(304px);\">\n",
      "       </div>\n",
      "       <div data-index=\"16\" style=\"width: 304px; left: -4864px; transition-duration: 0ms; transform: translateX(-304px);\">\n",
      "       </div>\n",
      "      </div>\n",
      "     </div>\n",
      "     <div class=\"slider-back-arrow icom-\">\n",
      "     </div>\n",
      "     <div class=\"slider-forward-arrow icom-\">\n",
      "     </div>\n",
      "    </a>\n",
      "   </div>\n",
      "   <div class=\"dots\">\n",
      "    <span class=\"dot selected\">\n",
      "     •\n",
      "    </span>\n",
      "    <span class=\"dot\">\n",
      "     •\n",
      "    </span>\n",
      "    <span class=\"dot\">\n",
      "     •\n",
      "    </span>\n",
      "    <span class=\"dot\">\n",
      "     •\n",
      "    </span>\n",
      "    <span class=\"dot\">\n",
      "     •\n",
      "    </span>\n",
      "    <span class=\"dot\">\n",
      "     •\n",
      "    </span>\n",
      "    <span class=\"dot\">\n",
      "     •\n",
      "    </span>\n",
      "    <span class=\"dot\">\n",
      "     •\n",
      "    </span>\n",
      "    <span class=\"dot\">\n",
      "     •\n",
      "    </span>\n",
      "    <span class=\"dot\">\n",
      "     •\n",
      "    </span>\n",
      "    <span class=\"dot\">\n",
      "     •\n",
      "    </span>\n",
      "    <span class=\"dot\">\n",
      "     •\n",
      "    </span>\n",
      "    <span class=\"dot\">\n",
      "     •\n",
      "    </span>\n",
      "    <span class=\"dot\">\n",
      "     •\n",
      "    </span>\n",
      "    <span class=\"dot\">\n",
      "     •\n",
      "    </span>\n",
      "    <span class=\"dot\">\n",
      "     •\n",
      "    </span>\n",
      "    <span class=\"dot\">\n",
      "     •\n",
      "    </span>\n",
      "   </div>\n",
      "  </div>\n",
      "  <a class=\"cl-app-anchor text-only posting-title\" href=\"https://philadelphia.craigslist.org/apa/d/philadelphia-beautiful-single-family/7800059598.html\" tabindex=\"0\">\n",
      "   <span class=\"label\">\n",
      "    Beautiful Single family Home - Cedar Park University City\n",
      "   </span>\n",
      "  </a>\n",
      "  <div class=\"meta\">\n",
      "   11 mins ago\n",
      "   <span class=\"separator\">\n",
      "   </span>\n",
      "   <span class=\"housing-meta\">\n",
      "    <span class=\"post-bedrooms\">\n",
      "     3br\n",
      "    </span>\n",
      "    <span class=\"post-sqft\">\n",
      "     1200ft\n",
      "     <span class=\"exponent\">\n",
      "      2\n",
      "     </span>\n",
      "    </span>\n",
      "   </span>\n",
      "   <span class=\"separator\">\n",
      "   </span>\n",
      "   Cedar park Philadelphia University City\n",
      "  </div>\n",
      "  <span class=\"priceinfo\">\n",
      "   $2,200\n",
      "  </span>\n",
      "  <button class=\"bd-button cl-favorite-button icon-only\" tabindex=\"0\" title=\"add to favorites list\" type=\"button\">\n",
      "   <span class=\"icon icom-\">\n",
      "   </span>\n",
      "  </button>\n",
      "  <button class=\"bd-button cl-banish-button icon-only\" tabindex=\"0\" title=\"hide posting\" type=\"button\">\n",
      "   <span class=\"icon icom-\">\n",
      "   </span>\n",
      "  </button>\n",
      " </div>\n",
      "</li>\n",
      "\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "element = propertySoup.select(\"li.cl-search-result\")\n",
    "print(element[0].prettify())\n",
    "print(len(element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Find the relevant pieces of information\n",
    "\n",
    "We will now focus on the **first element** in the list of 120 apartments. Use the `prettify()` function to print out the HTML for this first element. \n",
    "\n",
    "From this HTML, identify the HTML elements that hold:\n",
    "\n",
    "- The apartment price\n",
    "- The number of bedrooms\n",
    "- The square footage\n",
    "- The apartment title\n",
    "\n",
    "For the first apartment, print out each of these pieces of information, using BeautifulSoup to select the proper elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apartment price: $2,200\n",
      "number of bedrooms: 3\n",
      "square footage: 120\n",
      "apartment title: Beautiful Single family Home - Cedar Park University City\n"
     ]
    }
   ],
   "source": [
    "apt1 = element[0]\n",
    "spans = apt1.select('span')\n",
    "print(f\"apartment price: {apt1.find('span',{'class' : 'priceinfo'}).text}\",end=\"\\n\")\n",
    "\n",
    "print(f\"number of bedrooms: {apt1.find('span',{'class' : 'post-bedrooms'}).text[:1]}\",end=\"\\n\")\n",
    "\n",
    "print(f\"square footage: {apt1.find('span',{'class' : 'post-sqft'}).text[:3]}\",end=\"\\n\")\n",
    "\n",
    "print(f\"apartment title: {apt1.find('span',{'class' : 'label'}).text}\",end=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Functions to format the results \n",
    "\n",
    "In this section, you'll create functions that take in the raw string elements for price, size, and number of bedrooms and returns them formatted as numbers.\n",
    "\n",
    "I've started the functions to format the values. You should finish theses functions in this section.\n",
    "\n",
    "**Hints**\n",
    "- You can use string formatting functions like `string.replace()` and `string.strip()`\n",
    "- The `int()` and `float()` functions can convert strings to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_bedrooms(bedrooms_string):\n",
    "    # Format the bedrooms string and return an int\n",
    "    # \n",
    "    # This will involve using the string.replace() function to \n",
    "    # remove unwanted characters\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_size(size_string):\n",
    "    # Format the size string and return a float\n",
    "    # \n",
    "    # This will involve using the string.replace() function to \n",
    "    # remove unwanted characters\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_price(price_string):\n",
    "    # Format the price string and return a float\n",
    "    # \n",
    "    # This will involve using the string.strip() function to \n",
    "    # remove unwanted characters\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Putting it all together\n",
    "\n",
    "In this part, you'll complete the code block below using results from previous parts. The code will loop over 5 pages of search results and scrape data for 600 apartments. \n",
    "\n",
    "We can get a specific page by changing the `search=PAGE` part of the URL hash. For example, to get page 2 instead of page 1, we will navigate to:\n",
    "\n",
    "\n",
    "[https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=2~gallery~0~0](https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=2~gallery~0~0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, the outer for loop will loop over 5 pages of search results. The inner for loop will loop over the 120 apartments listed on each search page.\n",
    "\n",
    "Fill in the missing pieces of the inner loop using the code from the previous section. We will be able to extract out the relevant pieces of info for each apartment. \n",
    "\n",
    "After filling in the missing pieces and executing the code cell, you should have a Data Frame called `results` that holds the data for 600 apartment listings.\n",
    "\n",
    "#### Notes\n",
    "\n",
    "Be careful if you try to scrape more listings. Craigslist will temporarily ban your IP address (for a very short time) if you scrape too much at once. I've added a `sleep()` function to the for loop to wait 30 seconds between scraping requests.\n",
    "\n",
    "If the for loop gets stuck at the \"Processing page X...\" step for more than a minute or so, your IP address is probably banned temporarily, and you'll have to wait a few minutes before trying again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# search in batches of 120 for 5 pages\n",
    "# NOTE: you will get temporarily banned if running more than ~5 pages or so\n",
    "# the API limits are more leninient during off-peak times, and you can try\n",
    "# experimenting with more pages\n",
    "max_pages = 5\n",
    "\n",
    "# The base URL we will be using\n",
    "base_url = \"https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1\"\n",
    "\n",
    "# loop over each page of search results\n",
    "for page_num in range(1, max_pages + 1):\n",
    "    print(f\"Processing page {page_num}...\")\n",
    "\n",
    "    # Update the URL hash for this page number and make the combined URL\n",
    "    url_hash = f\"#search={page_num}~gallery~0~0\"\n",
    "    url = base_url + url_hash\n",
    "\n",
    "    # Go to the driver and wait for 5 seconds\n",
    "    driver.get(url)\n",
    "    sleep(5)\n",
    "\n",
    "    # YOUR CODE: get the list of all apartments\n",
    "    # This is the same code from Part 1.2 and 1.3\n",
    "    # It should be a list of 120 apartments\n",
    "    soup = \n",
    "    apts = \n",
    "    print(\"Number of apartments = \", len(apts))\n",
    "\n",
    "    # loop over each apartment in the list\n",
    "    page_results = []\n",
    "    for apt in apts:\n",
    "\n",
    "        # YOUR CODE: the bedrooms string\n",
    "        bedrooms = \n",
    "\n",
    "        # YOUR CODE: the size string\n",
    "        size = \n",
    "\n",
    "        # YOUR CODE: the title string\n",
    "        title = \n",
    "\n",
    "        # YOUR CODE: the price string\n",
    "        price = \n",
    "\n",
    "\n",
    "        # Format using functions from Part 1.5\n",
    "        bedrooms = format_bedrooms(bedrooms)\n",
    "        size = format_size(size)\n",
    "        price = format_price(price)\n",
    "\n",
    "        # Save the result\n",
    "        page_results.append([price, size, bedrooms, title])\n",
    "\n",
    "    # Create a dataframe and save\n",
    "    col_names = [\"price\", \"size\", \"bedrooms\", \"title\"]\n",
    "    df = pd.DataFrame(page_results, columns=col_names)\n",
    "    results.append(df)\n",
    "\n",
    "    print(\"sleeping for 10 seconds between calls\")\n",
    "    sleep(10)\n",
    "\n",
    "# Finally, concatenate all the results\n",
    "results = pd.concat(results, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Plotting the distribution of prices\n",
    "\n",
    "Use matplotlib's `hist()` function to make two histograms for:\n",
    "\n",
    "- Apartment prices\n",
    "- Apartment prices per square foot (price / size)\n",
    "\n",
    "Make sure to add labels to the respective axes and a title describing the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side note: rental prices per sq. ft. from Craigslist\n",
    "\n",
    "The histogram of price per sq ft should be centered around ~1.5. Here is a plot of how Philadelphia's rents compare to the other most populous cities:\n",
    "\n",
    "<img src=\"imgs/rental_prices_psf.png\" width=600/>\n",
    "\n",
    "[Source](https://arxiv.org/pdf/1605.05397.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Comparing prices for different sizes \n",
    "\n",
    "Use `altair` to explore the relationship between price, size, and number of bedrooms. Make an interactive scatter plot of price (x-axis) vs. size (y-axis), with the points colored by the number of bedrooms. \n",
    "\n",
    "Make sure the plot is interactive (zoom-able and pan-able) and add a tooltip with all of the columns in our scraped data frame.\n",
    "\n",
    "With this sort of plot, you can quickly see the outlier apartments in terms of size and price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Text Analytics\n",
    "\n",
    "- In this exercise, we'll explore sentiment analysis using Google review data for Wissahickon Valley Park. \n",
    "- You'll be able to apply a simple emotion analysis model and visualize the emotions expressed by visitors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Load Data Google Review Data\n",
    "\n",
    "To begin, let's load a dataset of Google reviews for Wissahickon Valley Park. This dataset includes user reviews, ratings, and other information, which we'll use to analyze visitors' emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv('wissahickon_review.csv')\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Apply Emotion Analysis Model \n",
    "\n",
    "Now, let's apply an emotion analysis model to understand the emotions conveyed in the reviews. We'll use a pre-trained model from Hugging Face.\n",
    "\n",
    "*Note1: Use the link below to access the model or choose another from [Hugging Face](https://huggingface.co/welcome) if you prefer.*\n",
    "\n",
    "*[DistilBERT Emotion Model](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion)*\n",
    "\n",
    "*Note2: take the label with the highest score as the predicted label for each text.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Visualize the Emotion Breakdown\n",
    "\n",
    "To gain insights, let's visualize the breakdown of emotions expressed in the reviews. This will help us understand the general sentiment of park visitors and any prevailing emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Additional Exploration\n",
    "\n",
    "You can explore the data further by creating a word cloud of frequently mentioned words for a specific emotion. For example, if you want to focus on reviews with \"joy\" as the dominant emotion, what are the words that appear frequently in those reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
